{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnza0H4V3MSP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "import pandas_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leemos y procesamos las datos para unir la informacion estatica con la info transaccional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 160192,
     "status": "ok",
     "timestamp": 1575047274661,
     "user": {
      "displayName": "Juan Manuel Ciro Torres",
      "photoUrl": "",
      "userId": "12480051337774408405"
     },
     "user_tz": 300
    },
    "id": "j97lP9TM3MS-",
    "outputId": "9a03fb79-d35c-4b1c-a52f-b019eab045ee"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/DT19_Datos_Var_Rpta_train.csv')\n",
    "df_detail = pd.read_csv('data/DT19_Datos_transaccionales_train.csv')\n",
    "df_id = pd.read_csv('data/DT19_IDs_predict.csv')\n",
    "df_trans_predict = pd.read_csv('data/DT19_Datos_transaccionales_predict.csv')\n",
    "df['train'] = 1\n",
    "df_id['train'] = 0\n",
    "print(df_detail.shape, df_trans_predict.shape)\n",
    "print(df_id.shape, df.shape)\n",
    "df = pd.concat([df, df_id])\n",
    "df_detail = pd.concat([df_detail, df_trans_predict])\n",
    "df_meta = pd.read_csv('data/DT19_maestro_cdgtrn_cdgrpta.csv')\n",
    "df_meta.fillna(0, inplace=True)\n",
    "df_detail['cdgrpta'] = df_detail['cdgrpta'].astype(float)\n",
    "df_detail['cdgtrn'] = df_detail['cdgtrn'].astype(float)\n",
    "df_detail['cdgrpta'] = df_detail['cdgrpta'].astype(str)\n",
    "df_detail['cdgtrn'] = df_detail['cdgtrn'].astype(str)\n",
    "print (df_detail.shape)\n",
    "df_detail = pd.merge(df, df_detail, on=['id'])\n",
    "print (df_detail.shape)\n",
    "df_detail = pd.merge(df_detail, df_meta, on=['canal','disposit','cdgtrn','cdgrpta'])\n",
    "df_detail['fecha_trxn'] =  pd.to_datetime(df_detail['fecha_trxn'])\n",
    "df_detail['year'] = df_detail['fecha_trxn'].dt.year\n",
    "df_detail['month'] = df_detail['fecha_trxn'].dt.month\n",
    "df_detail['day'] = df_detail['fecha_trxn'].dt.day\n",
    "df_detail['quarter'] = df_detail['fecha_trxn'].dt.quarter\n",
    "df_detail['dayofweek'] = df_detail['fecha_trxn'].dt.dayofweek\n",
    "df_detail['hour'] = df_detail['fecha_trxn'].dt.hour\n",
    "df_detail['second'] = df_detail['fecha_trxn'].dt.second\n",
    "df_detail['weekofyear'] = df_detail['fecha_trxn'].dt.strftime(\"%V\")\n",
    "df_detail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardamos los dataframe finales para no tener que repetir este proceso en futuras cargas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle('df_detail.plk')\n",
    "pd.to_pickle('df_final.plk')\n",
    "pd.to_pickle('df.plk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leemos los dataframes preprocesados  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_detail = pd.read_pickle('df_detail.plk')\n",
    "df_final = pd.read_pickle('df_final.plk')\n",
    "df = pd.read_pickle('df.plk')\n",
    "print ( df.shape, df_final.shape)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_eda = df[df['train'] == 1]\n",
    "df_eda['f_analisis'] = df_eda['f_analisis'].astype(str)\n",
    "df_eda['year'] = [ value[:4] for value in df_eda['f_analisis']]\n",
    "df_eda['year'] = df_eda['year'].astype(int)\n",
    "df_eda['month'] = [ value[4:] for value in df_eda['f_analisis'] ]\n",
    "df_eda['month'] = df_eda['month'].astype(int)\n",
    "df_eda.fillna(0, inplace=True)\n",
    "profile = df_eda.profile_report(title='BancolombiaEDA')\n",
    "profile.to_file(output_file=\"bancolombiaEDA.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos las caracteristicas (feature engineer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupamiento de sesiones unicas por cada una de las variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUgxxjIF9nQ9"
   },
   "outputs": [],
   "source": [
    "columns = ['canal', 'disposit','grupo_descrp_trxn',\n",
    "           'descrip_trxn','clasif_trxn','clasif_cod_rpta','producto_asociado',\n",
    "          'descripcion_grupo', 'descrip_cod_rpta','grupo_modifcado','culpa_banco']\n",
    "\n",
    "df_final = df.copy()\n",
    "##Cantidad de sesiones unicas por canal, dispositivo\n",
    "for column in columns:\n",
    "    print (column)\n",
    "    df_final_group = df_detail.groupby(['id',column], as_index=False).agg({'sesion':'nunique'})\n",
    "    for value in df_final_group[column].unique():\n",
    "        data = df_final_group[df_final_group[column] == value]\n",
    "        data.drop([column], axis=1, inplace=True)\n",
    "        df_final = pd.merge(df_final, data, on=['id'], how='left')\n",
    "        df_final.fillna(0, inplace=True)\n",
    "        df_final.rename(columns={'sesion':str(column) + '_' + str(value) + '_cantidad_sesiones' }, inplace=True)\n",
    "\n",
    "        \n",
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculo de factores estadÃ­sticos sum, mean, max, min y std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Valor sum, max, min,mean y std del valor de transaccion\n",
    "sum_vlr = df_detail.groupby(['id'], as_index=False).sum()[['id','vlrtran', 'sesion', 'year', 'month',\n",
    "       'day', 'hour', 'minute', 'second', 'quarter', 'dayofweek']]\n",
    "df_final = pd.merge(df_final, sum_vlr, on=['id'], how='left')\n",
    "df_final.rename(columns={'vlrtran':'vlrtran_sum',\n",
    "                        'sesion':'sesion_sum',\n",
    "                        'year':'year_sum',\n",
    "                        'month':'month_sum',\n",
    "                        'day':'day_sum',\n",
    "                        'hour':'hour_sum',\n",
    "                        'minute':'minute_sum',\n",
    "                        'second':'second_sum',\n",
    "                        'quarter':'quarter_sum',\n",
    "                        'dayofweek':'dayofweek_sum'}, inplace=True)\n",
    "print(df_final.shape)\n",
    "min_vlr = df_detail.groupby(['id'], as_index=False).min()[['id','vlrtran', 'sesion', 'year', 'month',\n",
    "       'day', 'hour', 'minute', 'second', 'quarter', 'dayofweek']]\n",
    "df_final = pd.merge(df_final, min_vlr, on=['id'], how='left')\n",
    "df_final.rename(columns={'vlrtran':'vlrtran_min',\n",
    "                        'sesion':'sesion_min',\n",
    "                        'year':'year_min',\n",
    "                        'month':'month_min',\n",
    "                        'day':'day_min',\n",
    "                        'hour':'hour_min',\n",
    "                        'minute':'minute_min',\n",
    "                        'second':'second_min',\n",
    "                        'quarter':'quarter_min',\n",
    "                        'dayofweek':'dayofweek_min'}, inplace=True)\n",
    "print(df_final.shape)\n",
    "\n",
    "max_vlr = df_detail.groupby(['id'], as_index=False).max()[['id','vlrtran', 'sesion', 'year', 'month',\n",
    "       'day', 'hour', 'minute', 'second', 'quarter', 'dayofweek']]\n",
    "df_final = pd.merge(df_final, max_vlr, on=['id'], how='left')\n",
    "df_final.rename(columns={'vlrtran':'vlrtran_max',\n",
    "                        'sesion':'sesion_max',\n",
    "                        'year':'year_max',\n",
    "                        'month':'month_max',\n",
    "                        'day':'day_max',\n",
    "                        'hour':'hour_max',\n",
    "                        'minute':'minute_max',\n",
    "                        'second':'second_max',\n",
    "                        'quarter':'quarter_max',\n",
    "                        'dayofweek':'dayofweek_max'}, inplace=True)\n",
    "print(df_final.shape)\n",
    "\n",
    "\n",
    "mean_vlr = df_detail.groupby(['id'], as_index=False).mean()[['id','vlrtran', 'sesion', 'year', 'month',\n",
    "       'day', 'hour', 'minute', 'second', 'quarter', 'dayofweek']]\n",
    "df_final = pd.merge(df_final, mean_vlr, on=['id'], how='left')\n",
    "df_final.rename(columns={'vlrtran':'vlrtran_mean',\n",
    "                        'sesion':'sesion_mean',\n",
    "                        'year':'year_mean',\n",
    "                        'month':'month_mean',\n",
    "                        'day':'day_mean',\n",
    "                        'hour':'hour_mean',\n",
    "                        'minute':'minute_mean',\n",
    "                        'second':'second_mean',\n",
    "                        'quarter':'quarter_mean',\n",
    "                        'dayofweek':'dayofweek_mean'}, inplace=True)\n",
    "print(df_final.shape)\n",
    "\n",
    "std_vlr = df_detail.groupby(['id'], as_index=False).std()[['id','vlrtran', 'sesion', 'year', 'month',\n",
    "       'day', 'hour', 'minute', 'second', 'quarter', 'dayofweek']]\n",
    "df_final = pd.merge(df_final, std_vlr, on=['id'], how='left')\n",
    "df_final.rename(columns={'vlrtran':'vlrtran_std',\n",
    "                        'sesion':'sesion_std',\n",
    "                        'year':'year_std',\n",
    "                        'month':'month_std',\n",
    "                        'day':'day_std',\n",
    "                        'hour':'hour_std',\n",
    "                        'minute':'minute_std',\n",
    "                        'second':'second_std',\n",
    "                        'quarter':'quarter_std',\n",
    "                        'dayofweek':'dayofweek_std'}, inplace=True)\n",
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##One-hot encoding de los segmentos\n",
    "df_ = df.copy()\n",
    "df_['segmento'] = np.where(df_['segmento'] == 0, 6, df_['segmento'])\n",
    "dummy = pd.get_dummies(df_['segmento'], prefix='segmento',  drop_first=True)\n",
    "df_ = pd.concat([df[['id']], dummy], axis=1)\n",
    "df_final = pd.merge(df_final, df_, on=['id'], how='left')\n",
    "\n",
    "print(df_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculado diferencia de fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['f_analisis'] = df_final['f_analisis'].astype(str)\n",
    "df_final['year'] = [ value[:4] for value in df_final['f_analisis']]\n",
    "df_final['year'] = df_final['year'].astype(int)\n",
    "df_final['month'] = [ value[4:] for value in df_final['f_analisis'] ]\n",
    "df_final['month'] = df_final['month'].astype(int)\n",
    "df_final.fillna(0, inplace=True)\n",
    "\n",
    "##Diff fechas\n",
    "df_final['max_diff_year'] = df_final['year'] - df_final['year_max']\n",
    "df_final['min_diff_year'] = df_final['year'] - df_final['year_min']\n",
    "df_final['mean_diff_year'] = df_final['year'] - df_final['year_mean']\n",
    "df_final['max_diff_month'] = df_final['month'] - df_final['month_max']\n",
    "df_final['min_diff_month'] = df_final['month'] - df_final['month_min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porcentage de cada una de las posibles salidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_detail = df_detail[\"var_rpta\"].value_counts(normalize=True)[1]\n",
    "no_fail_detail = df_detail[\"var_rpta\"].value_counts(normalize=True)[0]\n",
    "df_final['fail_detail'] = np.where(df_final['var_rpta'] == 1, fail_detail, fail_detail)\n",
    "df_final['no_fail_detail'] = np.where(df_final['var_rpta'] == 0, no_fail_detail, no_fail_detail)\n",
    "fail_detail = df[\"var_rpta\"].value_counts(normalize=True)[1]\n",
    "no_fail_detail = df[\"var_rpta\"].value_counts(normalize=True)[0]\n",
    "df_final['percentage_fail'] = np.where(df_final['var_rpta'] == 1, fail_detail, fail_detail)\n",
    "df_final['percentage_no_fail'] = np.where(df_final['var_rpta'] == 0, no_fail_detail, no_fail_detail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3MAZkGG_Utoy"
   },
   "source": [
    "### Importancia de las variables para clusterizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 707704,
     "status": "ok",
     "timestamp": 1575001620561,
     "user": {
      "displayName": "Juan Manuel Ciro Torres",
      "photoUrl": "",
      "userId": "12480051337774408405"
     },
     "user_tz": 300
    },
    "id": "11MZUHMgJDCu",
    "outputId": "78455d77-ad35-4c05-ef2e-f79b50103ad6"
   },
   "outputs": [],
   "source": [
    "model = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=2,\n",
    "    seed=27)\n",
    "model.fit(X_sc,y)\n",
    "score_train = model.score(X_train,y_train)\n",
    "score_test = model.score(X_test,y_test)\n",
    "print(\"Score train is: {} and Score test is {}\".format(score_train, score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 707647,
     "status": "ok",
     "timestamp": 1575001620562,
     "user": {
      "displayName": "Juan Manuel Ciro Torres",
      "photoUrl": "",
      "userId": "12480051337774408405"
     },
     "user_tz": 300
    },
    "id": "ob3zZHI-IjUb",
    "outputId": "0729f4e9-eb7c-4453-9f51-33ed87014bc3"
   },
   "outputs": [],
   "source": [
    "#Mirando las variables explicativas\n",
    "predictors = X.columns\n",
    "features = list(zip(predictors,model.feature_importances_))\n",
    "variables = pd.DataFrame(np.array(features).reshape(X.shape[1],2), columns = [\"Variable\",\"Peso\"])\n",
    "variables['Peso'] = variables['Peso'].astype(float)\n",
    "variables.sort_values([\"Peso\"], ascending=False, inplace=True)\n",
    "variables.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentacion propia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cluster con k-means\n",
    "variable = list(variables.head(80)['Variable'].values)\n",
    "variable.append('id')\n",
    "variable.append('var_rpta')\n",
    "df_final_t = df_final[variable].groupby('id').sum()\n",
    "X = df_final_t\n",
    "\n",
    "X.dropna(inplace=True)\n",
    "sc = MinMaxScaler()\n",
    "X_sc = sc.fit_transform(X)\n",
    "\n",
    "##Agrupamiento por KMeans con 5 clusters\n",
    "km = KMeans(n_clusters=250, random_state=5)\n",
    "km.fit(X_sc)\n",
    "labels = km.labels_\n",
    "mm = pd.DataFrame(km.fit_predict(X), columns=['labels'])\n",
    "silhouette = (X, mm['labels'])\n",
    "\n",
    "# Agregar cluster a los clientes\n",
    "df_final_cluster = df_final.copy()\n",
    "df_final[\"Medium Cluster\"] = km.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_ = list(variables[variables['Peso'] > 0.005]['Variable'].values)\n",
    "df_final = df_final[variables_]\n",
    "df_final = pd.concat([df[['id','var_rpta','train']].reset_index(drop=True), df_final], axis=1)\n",
    "print(df_final.shape)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_ = df_final[df_final['train'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7a35ifvz3MW4"
   },
   "outputs": [],
   "source": [
    "X = df_final_.drop([\"var_rpta\",'id','f_analisis','train'],axis=1)\n",
    "y = df_final_['var_rpta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00jp6ojf3MXE"
   },
   "outputs": [],
   "source": [
    "#Estandarizando los datos\n",
    "sc = StandardScaler()\n",
    "X_sc = sc.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sc, y,shuffle=False,test_size=0.2)\n",
    "models = [LogisticRegression(), RandomForestClassifier(), XGBClassifier(), AdaBoostClassifier()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1kNNits-UzUU"
   },
   "source": [
    "## Evaluacion de los modelos posibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 168746,
     "status": "ok",
     "timestamp": 1575047929208,
     "user": {
      "displayName": "Juan Manuel Ciro Torres",
      "photoUrl": "",
      "userId": "12480051337774408405"
     },
     "user_tz": 300
    },
    "id": "D2aTGGn73MXQ",
    "outputId": "ad99282c-c47d-401b-ae9d-d402656eb8dd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_score = []\n",
    "for model in models:\n",
    "    model = model\n",
    "    score_model = cross_val_score(model,X_sc,y, cv=2,scoring='roc_auc')\n",
    "    score_final = score_model.mean()\n",
    "    cv_score.append(score_final)\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRVsMAn53MWn"
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = []\n",
    "model_e = XGBClassifier()\n",
    "score_model = cross_val_score(model_e,X_sc,y, cv=3,scoring='roc_auc')\n",
    "score_final = score_model.mean()\n",
    "cv_score.append(score_final)\n",
    "print(str(cv_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbounds = {\n",
    "    'learning_rate': (0.01, 1.0),\n",
    "    'n_estimators': (100, 1000),\n",
    "    'max_depth': (3,10),\n",
    "    'subsample': (0.5, 1.0),  # Change for big datasets\n",
    "    'colsample': (0.5, 1.0),  # Change for datasets with lots of features\n",
    "    'gamma': (0, 5)}\n",
    " \n",
    "def xgboost_hyper_param(learning_rate,\n",
    "                        n_estimators,\n",
    "                        max_depth,\n",
    "                        subsample,\n",
    "                        colsample,\n",
    "                        gamma):\n",
    " \n",
    "    max_depth = int(max_depth)\n",
    "    n_estimators = int(n_estimators)\n",
    " \n",
    "    clf = XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        gamma=gamma)\n",
    "    return np.mean(cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc'))\n",
    " \n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgboost_hyper_param,\n",
    "    pbounds=pbounds,\n",
    "    random_state=1,\n",
    ")\n",
    "optimizer.maximize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtEEHyM6U6-L"
   },
   "source": [
    "## Prediccion de los nuevos valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_p = df_final[df_final['train'] == 0]\n",
    "X_p = df_final_p.drop([\"var_rpta\",'f_analisis','id','train'],axis=1)\n",
    "y_p = df_final_p['var_rpta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_score = []\n",
    "\n",
    "model_e = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=1,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=2,\n",
    "    seed=27)\n",
    "score_model = cross_val_score(model_e,X_sc,y, cv=3,scoring='roc_auc')\n",
    "score_final = score_model.mean()\n",
    "cv_score.append(score_final)\n",
    "print(str(cv_score))\n",
    "model_e.fit(X_sc, y, eval_metric=['auc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estandarizando los datos\n",
    "sc = StandardScaler()\n",
    "X_sc_p = sc.fit_transform(X_p)\n",
    "y_pred = model_e.predict_proba(X_sc_p)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['Pago','probabilidad'])\n",
    "df_id = pd.read_csv('data/DT19_IDs_predict.csv')\n",
    "y_pred = pd.concat([df_id[['id']], y_pred[['probabilidad']]], axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "td00JKMaMpXp"
   },
   "outputs": [],
   "source": [
    "y_pred.to_csv('try1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "EDA 2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
